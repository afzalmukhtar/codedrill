/**
 * Ollama Provider
 *
 * Implements LLMProvider for local Ollama instances.
 * Zero-cost, fully private, no API key needed.
 *
 * Features:
 * - Auto-detect if Ollama is running on localhost
 * - Auto-discover installed models via Ollama API
 * - Streaming chat completions
 * - Configurable base URL for remote Ollama instances
 */

// import { LLMProvider, ModelInfo, ChatRequest, ChatChunk } from "./types";

export class OllamaProvider /* implements LLMProvider */ {
  // TODO: constructor(baseUrl: string)
  // TODO: getAvailableModels(): Promise<ModelInfo[]>
  // TODO: chat(params: ChatRequest): AsyncIterable<ChatChunk>
  // TODO: isAvailable(): Promise<boolean>
}
